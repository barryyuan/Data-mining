

1.  Given the training data in the table below, predict the class of the following new example using Naïve Bayes classification: 
          New example: age=youth, income=medium, student=yes, credit-rating=fair 
 
According to the dataset,
 
 

Suppose X={age=youth, income=medium, student=yes, credit-rating=fair}
 
 
 
Therefore, the class of the new sample is yes.

2. Consider the one-dimensional data set shown in the following Table

 
a) Classify the data point x = 5.0 according to its 1−, 3−, 5−, and 9−nearest neighbors (hand computation). 
b) Use KNN function in the R package “class” to classify the data point x=5.0. Suppose K=3. 
library(class)
y=c("-","-","+","+","+","-","-","+","-","-")
tx=c(0.5,3.0,4.5,4.6,4.9,5.2,5.3,5.5,7.0,9.5)
cx=5.0
cc<-knn(tx, cx, y,k = 3)
c) Write you own R program which will use 5-nearset neighbors to classify the data point x=5.0. 
y=c("-","-","+","+","+","-","-","+","-","-")
x=c(0.5,3.0,4.5,4.6,4.9,5.2,5.3,5.5,7.0,9.5)
dis=abs(x-5)
a=sort.list(dis)[1:5]
Class=ifelse(sum(y[a]=="+")>sum(y[a]=="-"),"+", "-")
3.
a)	Select randomly two-thirds of Titanic passengers as training data and the 
remaining one-thirds as testing data. Apply naïve Bayes method to classify testing data. Give the misclassification error rate.
library(class)
install.packages("e1071")
library(e1071)
titanic=read.csv(file.choose(),header=T)
d=dim(titanic)
titanic[,d[2]]=as.factor(titanic[,d[2]])
a=sample(1:d[1],d[1]*2/3)
traindata=titanic[a,]
testdata=titanic[-a,]
m = naiveBayes(survived ~ ., data = traindata)
pc=predict(m,testdata[,1:3])
test.error=sum(pc!=testdata[,d[2]])/dim(testdata)[1]
test.error
[1] 0.2384196
b) Instead of using library e1071, write your own R program which use Naive Bayes classification method to solve problem a). 
a1=levels(traindata[,1])
 a2=levels(traindata[,2])
 a3=levels(traindata[,3])
 a4=levels(traindata[,4])
 level=list(a1,a2,a3,a4)
 b1=matrix(rep(0,8),nrow=2)        # to record the P(X|Y)
 b2=matrix(rep(0,4),nrow=2)
 b3=matrix(rep(0,4),nrow=2)
 b4=matrix(rep(0,2),nrow=2)
 p=list(b1,b2,b3,b4)
 p[[4]][1]=length(which(traindata[,4]==level[[4]][1]))/d[1]  # percentage of each class of Y
 p[[4]][2]=length(which(traindata[,4]==level[[4]][2]))/d[1]
 for(i in 1:3) 
 {for(j in 1:length(level[[i]]))
{p[[i]][1,j]=length(which(traindata[,i]==level[[i]][j]&traindata[,4]==level[[4]][1]))/length(which(traindata[,4]==level[[4]][1]))
 p[[i]][2,j]=length(which(traindata[,i]==level[[i]][j]&traindata[,4]==level[[4]][2]))/length(which(traindata[,4]==level[[4]][2]))
 }}
 o=dim(testdata)[1]*4
 l=matrix(rep(0,o),ncol=4)   
 for(k in 1:dim(testdata)[1])
 {for(i in 1:4)
 {l[k,i]=which(testdata[k,i]==level[[i]])  #Identify the level of value of variable X
 }}
 pc=vector()
 for(k in 1:dim(testdata)[1])
 {
 pk1=p[[1]][1,l[k,1]]*p[[2]][1,l[k,2]]*p[[3]][1,l[k,3]]*p[[4]][1]  #P(X1|Y)P(X2|Y)P(X3|Y)P(Y)
 pk2=p[[1]][2,l[k,1]]*p[[2]][2,l[k,2]]*p[[3]][2,l[k,3]]*p[[4]][2]
 pc[k]=ifelse(pk1>pk2,0,1)
 }
 test.error=length(which(pc!=testdata[,4]))/dim(testdata)[1]
 test.error
[1] 0.2384196

4.	The diabetes data set is taken from the UCI machine learning database repository at: http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes
•	768 samples in the dataset
•	8 quantitative variables
•	2 classes; with or without signs of diabetes
For the convenience of visualization, we take the first two principle components as the new feature variables and conduct k-NN only on these two dimensional data. Use R to perform KNN and determine the optimal k by using 10-fold cross validation. Plot the test misclassification rates based on different k versus the number of neighbors k. If possible, draw the classification boundaries on the scatter plot based on the two principle components. 

diabetes=read.csv(file.choose(), header=F)
d=dim(diabetes)
diabetes1=diabetes[,-d[2]] 
PCA=princomp(diabetes1,cor=TRUE,scores=TRUE)
diabetesnew=data.frame(PCA$scores[,1:2],diabetes[,d[2]])
n=10
ktest.error=vector()
for (k in (1:100))
{
pc=vector()
num<-d[1]%/%n
test.error<-vector()	
for (i in (1: n))
{if (i!=n) 
a<-((i-1)*num+1): (i*num)
if (i==n) 
a<-((i-1)*num+1):d[1]
traindata<-diabetesnew[-a,]
testdata<-diabetesnew[a,]
pc=knn(traindata[,1:2],testdata[,1:2],traindata[,3],k)
test.error[i]=sum(pc!=testdata[,3])/num
 }
ktest.error[k]=mean(test.error)
 }
kchoose=which.min(ktest.error)
kchoose
[1] 39
plot(ktest.error,ylab="test misclassification rates",xlab="k")
 

pc1=seq(minscore[1], maxscore[1], by=0.1)
pc2=seq(minscore[2], maxscore[2], by=0.1)
lpc1=length(pc1)
lpc2=length(pc2)
newpc1=rep(pc1,rep(lpc2,lpc1))
newpc2=rep(pc2,lpc1)
newdata=data.frame(newpc1,newpc2)
alldata=data.frame(newpc1,newpc2,predictclass=knn(diabetesnew[,1:2],newdata,diabetesnew[,3],39))
color1=rep('red',dim(alldata)[1])
color1[alldata[,3]==0]='blue'
plot(alldata[,1:2],col=color1,cex=0.5,pch=21)
legend("bottomleft",c("class-0","class-1"),col=c("blue","red"),pch=21)
 

